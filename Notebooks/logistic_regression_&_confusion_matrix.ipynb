{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = pd.read_csv('../Datasets/diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break data into test & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    130\n",
       "1     62\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pima = pd.read_csv('../Datasets/diabetes.csv')\n",
    "\n",
    "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\n",
    "\n",
    "# X is a matrix,access the features we want in feature_cols\n",
    "X = pima[feature_cols]\n",
    "\n",
    "# y is a vector, hence we use dot to access 'label'\n",
    "y = pima['Outcome']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See what the model predicts for y predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: write a function that counts the following:\n",
    "- how many 0s in y_pred are 0s in y_test\n",
    "- how many 1s in y_pred are 1s in y_test\n",
    "- how many 0s in y_pred are 1s in y_test\n",
    "- how many 1s in y_pred are 0s in y_test\n",
    "\n",
    "It should return these four values in that order\n",
    "\n",
    "This will give us the values for our confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118,  12],\n",
       "       [ 47,  15]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def confusion_matrix(predicted_values, test_values):\n",
    "    actual_zeroes = 0\n",
    "    actual_ones = 0\n",
    "    false_zeroes = 0\n",
    "    false_ones = 0\n",
    "    for predicted_value, test_value in zip(predicted_values, test_values):\n",
    "        if predicted_value == 0 and test_value == 0:\n",
    "            actual_zeroes += 1\n",
    "        elif predicted_value == 1 and test_value == 1:\n",
    "            actual_ones += 1\n",
    "        elif predicted_value == 0 and test_value == 1:\n",
    "            false_zeroes += 1\n",
    "        elif predicted_value == 1 and test_value == 0:\n",
    "            false_ones += 1\n",
    "    return np.array([[actual_zeroes, false_ones],[false_zeroes, actual_ones]])\n",
    "\n",
    "confusion_matrix(y_pred, y_test.values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is like a punnet square from biology.\n",
    "These describe the real value vs the predicted value.\n",
    "It us good when the expected is the same as the real value.\n",
    "This means we like when the points 0,0 and 1,1 are greater.\n",
    "Optimally we want 1,0 and 0,1 to be 1. This is a perfect model.\n",
    "\n",
    "|   | 0 | 1 |\n",
    "|:--|:--|:--|\n",
    "| 0 |118| 12|\n",
    "| 1 | 47| 15|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118,  12],\n",
       "       [ 47,  15]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# using sklearn's metrics we can create a confusion matrix that is given in a 2D array\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels for a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True positive - when we expect a positive result and we get one. So if we expect 1 we get 1.\n",
    "- True negative - the opposite of true positive, that is if we expect 0 we get 0.\n",
    "- False positive - when we expect a positive result but recieve a negative one. So if we expect 1 and get 0.\n",
    "- False negative - the opposite of false positives, that is if we expect 0 we get 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # TRUE POSITIVE\n",
    "TN = confusion[0,0] # TRUE NEGATIVE\n",
    "FP = confusion[0,1] # FALSE POSITIVE\n",
    "FN = confusion[1,0] # FASLE NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing multidimentional arrays normally is done like so.\n",
    "arr_2d = [['00','01'],['10','11']]\n",
    "arr_2d[1][0] # returns '10'\n",
    "\n",
    "# Numpy arrays offers a cleaner way of array indexing.\n",
    "np_2d = np.array(arr_2d)\n",
    "np_2d[1,0] # returns '10'\n",
    "np_2d[1][0] # also works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a confusion matrix we can calculate three things:\n",
    "- Accuracy - The total models accuracy. The percentage of predictions that are correct.\n",
    "- Precision - The percentage of positive predictions that were predicted correctly.\n",
    "- Recall - The percentage of actual positives that were predicted correctly.\n",
    "- F1-score - harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 \n",
    "\n",
    "##### Finding Accuracy \n",
    "- (TN + TP) / (TP + TN + FP + FN)\n",
    "\n",
    "##### Finding Precision \n",
    "- TP / (TP + FP)\n",
    "\n",
    "##### Finding Recall \n",
    "- TP / (TP + FN)\n",
    "\n",
    "##### F1-score\n",
    "- 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927083333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_accuracy(confusion_matrix):\n",
    "    TP = confusion_matrix[1,1]\n",
    "    TN = confusion_matrix[0,0]\n",
    "    return (TP + TN) / sum(sum(confusion_matrix))\n",
    "\n",
    "model_accuracy(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_precision(confusion_matrix):\n",
    "    TP = confusion_matrix[1,1]\n",
    "    FP = confusion_matrix[0,1]\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "model_precision(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24193548387096775"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_recall(confusion_matrix):\n",
    "    TP = confusion_matrix[1,1]\n",
    "    FN = confusion_matrix[1,0]\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "model_recall(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3370786516853933"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f1_score(confusion_matrix):\n",
    "    precision = model_precision(confusion_matrix)\n",
    "    recall = model_recall(confusion_matrix)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "f1_score(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_accuracy 0.6510416666666666\n",
      "model_precision 0.4691358024691358\n",
      "model_recall 0.6129032258064516\n",
      "f1_score 0.5314685314685315\n"
     ]
    }
   ],
   "source": [
    "# threshold = (num of 1s) / total\n",
    "threshold = sum([outcome == 1 for outcome in y_train]) / len(y_train)\n",
    "\n",
    "# all predicted probablities of x_test\n",
    "x_text_predicted_probs = logreg.predict_proba(X_test)\n",
    "\n",
    "# \n",
    "new_y_train = []\n",
    "for prob in x_text_predicted_probs:\n",
    "    if prob[1] > threshold:\n",
    "        new_y_train.append(1)\n",
    "    else:\n",
    "        new_y_train.append(0)\n",
    "\n",
    "confusion = confusion_matrix(new_y_train, y_test)\n",
    "\n",
    "print('model_accuracy' , model_accuracy(confusion))\n",
    "print('model_precision' , model_precision(confusion))\n",
    "print('model_recall' , model_recall(confusion))\n",
    "print('f1_score' , f1_score(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
